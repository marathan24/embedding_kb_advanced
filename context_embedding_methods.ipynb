{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e64db33-4119-4fa1-aae6-8c284855ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install numpy openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f730199-ebd4-4702-8b8d-d2652d12552d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your open ai key here:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your open ai key here: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c441536-59e5-4bcb-9944-b5a890d4a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf57e45-1e36-4cb9-92c6-a4f3bcc65748",
   "metadata": {},
   "source": [
    "# Contextual Embedding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90296909-6ebd-4a1b-bd8a-9ef417bdf930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualEmbedder:\n",
    "    def __init__(self, model: str = \"text-embedding-3-small\"):\n",
    "        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.model = model\n",
    "        self.tokenizer = tiktoken.encoding_for_model(model)\n",
    "        \n",
    "    def chunk_by_sentences(self, input_text: str) -> Tuple[List[str], List[Tuple[int, int]]]:\n",
    "        \"\"\"\n",
    "        Split the input text into sentences and track token spans\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer.encode(input_text)\n",
    "        \n",
    "        chunk_positions = []\n",
    "        current_pos = 0\n",
    "        \n",
    "        for i in range(len(tokens)):\n",
    "            decoded_token = self.tokenizer.decode([tokens[i]])\n",
    "            if decoded_token == \".\" and i < len(tokens) - 1:\n",
    "                next_token = self.tokenizer.decode([tokens[i + 1]])\n",
    "                if next_token.startswith(\" \"):\n",
    "                    chunk_positions.append((current_pos, i + 1))\n",
    "                    current_pos = i + 1\n",
    "        \n",
    "        if current_pos < len(tokens):\n",
    "            chunk_positions.append((current_pos, len(tokens)))\n",
    "        \n",
    "        chunks = []\n",
    "        for start, end in chunk_positions:\n",
    "            chunk_tokens = tokens[start:end]\n",
    "            chunk_text = self.tokenizer.decode(chunk_tokens)\n",
    "            chunks.append(chunk_text)\n",
    "            \n",
    "        return chunks, chunk_positions\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a single piece of text\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.embeddings.create(\n",
    "                model=self.model,\n",
    "                input=text\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding: {e}\")\n",
    "            return []\n",
    "\n",
    "    def traditional_chunking(self, chunks: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Generate embeddings for each chunk independently\n",
    "        \"\"\"\n",
    "        return [self.embed_text(chunk) for chunk in chunks]\n",
    "\n",
    "    def contextual_chunking(self, input_text: str) -> Tuple[List[str], List[List[float]], List[List[float]]]:\n",
    "        \"\"\"\n",
    "        Perform both traditional and context-sensitive chunking\n",
    "        \"\"\"\n",
    "        chunks, span_annotations = self.chunk_by_sentences(input_text)\n",
    "        \n",
    "        traditional_embeddings = self.traditional_chunking(chunks)\n",
    "        \n",
    "        full_text_embedding = self.embed_text(input_text)\n",
    "        \n",
    "        contextual_embeddings = []\n",
    "        embedding_dim = len(full_text_embedding)\n",
    "        \n",
    "        for start, end in span_annotations:\n",
    "            chunk_length = end - start\n",
    "            weights = np.ones(embedding_dim) * (1.0 / chunk_length)\n",
    "            chunk_embedding = np.multiply(full_text_embedding, weights)\n",
    "            contextual_embeddings.append(chunk_embedding.tolist())\n",
    "        \n",
    "        return chunks, contextual_embeddings, traditional_embeddings\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between two vectors\n",
    "        \"\"\"\n",
    "        vec1 = np.array(vec1)\n",
    "        vec2 = np.array(vec2)\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba439928-09b4-43c5-9bb7-3f19899ad3da",
   "metadata": {},
   "source": [
    "# Embedder Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abff3ec2-870c-4b94-a7c6-6488d087a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = ContextualEmbedder()\n",
    "\n",
    "input_text = \"\"\"Berlin is the capital and largest city of Germany, both by area and by population. \n",
    "Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits. \n",
    "The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa39971-38e2-44fa-bc6a-ac56b02e054c",
   "metadata": {},
   "source": [
    "# FINAL RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3db58d49-f92b-4f94-be1e-8992c615b8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: \"Berlin is the capital and largest city of Germany, both by area and by population.\"\n",
      "Contextual similarity: 0.4382\n",
      "Traditional similarity: 0.4268\n",
      "\n",
      "Chunk: \" \n",
      "Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"\n",
      "Contextual similarity: 0.4382\n",
      "Traditional similarity: 0.2534\n",
      "\n",
      "Chunk: \" \n",
      "The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\n",
      "Contextual similarity: 0.4382\n",
      "Traditional similarity: 0.3479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunks, contextual_emb, traditional_emb = embedder.contextual_chunking(input_text)\n",
    "\n",
    "berlin_embedding = embedder.embed_text(\"Berlin\")\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    context_sim = embedder.cosine_similarity(berlin_embedding, contextual_emb[i])\n",
    "    trad_sim = embedder.cosine_similarity(berlin_embedding, traditional_emb[i])\n",
    "    \n",
    "    print(f'Chunk: \"{chunk}\"')\n",
    "    print(f'Contextual similarity: {context_sim:.4f}')\n",
    "    print(f'Traditional similarity: {trad_sim:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eac4ff-66a2-4208-a557-ab8acd5b2100",
   "metadata": {},
   "source": [
    "# METHOD 2 (Zero Entropy Repo -- The pdf text took 5 minutes to make chunks -- Not recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38b9f783-284b-4589-8375-77dac08b0621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a85d1cfc-80b2-48f0-91d8-f68fe04d7882",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChunkingConfig:\n",
    "    model: str = \"gpt-4o\"\n",
    "    split_threshold: float = -2.0  # More strict threshold like Llama taken from the zero-entropy repository\n",
    "    min_chunk_size: int = 100\n",
    "    max_chunk_size: int = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "062988c5-fd53-4b37-b9df-8654182dd9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedChunker:\n",
    "    def __init__(self, config: ChunkingConfig):\n",
    "        self.config = config\n",
    "        self.client = openai.OpenAI()\n",
    "        self.tokenizer = tiktoken.encoding_for_model(config.model)\n",
    "        \n",
    "    def get_split_decisions(self, text: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Get split decisions and logprobs using OpenAI's API\"\"\"\n",
    "        system_message = \"You are a document chunking assistant. Insert '段' at natural break points in the text.\"\n",
    "        prompt = f\"Add the '段' character at natural semantic break points in this text:\\n\\n{text}\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.config.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_message},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                logprobs=True,\n",
    "                top_logprobs=5,\n",
    "                temperature=0\n",
    "            )\n",
    "            \n",
    "            token_decisions = []\n",
    "            for token_info in response.choices[0].logprobs.content:\n",
    "                token = token_info.token\n",
    "                split_prob = float('-inf')\n",
    "                \n",
    "                for logprob in token_info.top_logprobs:\n",
    "                    if '段' in logprob.token:\n",
    "                        split_prob = logprob.logprob\n",
    "                        break\n",
    "                        \n",
    "                token_decisions.append((token, split_prob))\n",
    "            \n",
    "            return token_decisions\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting logprobs: {e}\")\n",
    "            return []\n",
    "\n",
    "    def chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Main chunking method working with tokens\"\"\"\n",
    "        print(\"Getting split decisions...\")\n",
    "        token_decisions = self.get_split_decisions(text)\n",
    "        \n",
    "        print(\"Creating chunks based on split decisions...\")\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_text = \"\"\n",
    "        \n",
    "        for token, split_prob in token_decisions:\n",
    "            current_chunk.append(token)\n",
    "            current_text += token\n",
    "            \n",
    "            should_split = (\n",
    "                split_prob > self.config.split_threshold and \n",
    "                len(current_text) >= self.config.min_chunk_size\n",
    "            ) or len(current_text) >= self.config.max_chunk_size\n",
    "            \n",
    "            if should_split:\n",
    "                chunk_text = ''.join(current_chunk).strip()\n",
    "                if chunk_text:\n",
    "                    chunks.append(chunk_text)\n",
    "                current_chunk = []\n",
    "                current_text = \"\"\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunk_text = ''.join(current_chunk).strip()\n",
    "            if chunk_text:\n",
    "                chunks.append(chunk_text)\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "    def analyze_chunks(self, chunks: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze the chunking results\"\"\"\n",
    "        lengths = [len(chunk) for chunk in chunks]\n",
    "        return {\n",
    "            \"num_chunks\": len(chunks),\n",
    "            \"avg_chunk_length\": np.mean(lengths),\n",
    "            \"min_chunk_length\": min(lengths),\n",
    "            \"max_chunk_length\": max(lengths),\n",
    "            \"total_characters\": sum(lengths)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c41fd4f-4948-4617-a6c8-80f605d50750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting split decisions...\n",
      "Creating chunks based on split decisions...\n",
      "\n",
      "Chunked Text:\n",
      "\n",
      "Chunk 1:\n",
      "Section 1.  \n",
      "All legislative Powers herein granted shall be vested in a Congress of the United States, which shall consist of a Senate and House of Representatives. \\xe6\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk 2:\n",
      "\\xae\\xb5\n",
      "\n",
      "Section 2.  \n",
      "The House of Representatives shall be composed of Members chosen every second Year by the People of the several States, and the Electors in each State shall have the Qualifications requisite for Electors of the most numerous Branch of the State Legislature. \\xe6\\xae\\xb5\n",
      "\n",
      "No Person shall be a Representative who shall not have attained to the Age of twenty five Years, and been seven Years a Citizen of the United States, and who shall not, when elected, be an Inhabitant of that State in which he shall be chosen.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunking Analysis:\n",
      "num_chunks: 2\n",
      "avg_chunk_length: 353.00\n",
      "min_chunk_length: 169\n",
      "max_chunk_length: 537\n",
      "total_characters: 706\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Section 1.\n",
    "All legislative Powers herein granted shall be vested in a Congress of the United States, which shall consist of a Senate and House of Representatives.\n",
    "\n",
    "Section 2.\n",
    "The House of Representatives shall be composed of Members chosen every second Year by the People of the several States, and the Electors in each State shall have the Qualifications requisite for Electors of the most numerous Branch of the State Legislature.\n",
    "\n",
    "No Person shall be a Representative who shall not have attained to the Age of twenty five Years, and been seven Years a Citizen of the United States, and who shall not, when elected, be an Inhabitant of that State in which he shall be chosen.\"\"\"\n",
    "\n",
    "config = ChunkingConfig()\n",
    "chunker = ImprovedChunker(config)\n",
    "chunks = chunker.chunk_text(text)\n",
    "\n",
    "print(\"\\nChunked Text:\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "analysis = chunker.analyze_chunks(chunks)\n",
    "print(\"\\nChunking Analysis:\")\n",
    "for key, value in analysis.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a254d9aa-258f-4d82-917b-936f96adf60f",
   "metadata": {},
   "source": [
    "# METHOD 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dbb0874-5940-406d-b5e5-bfdec4a22e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "from typing import List, Tuple, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1312f469-8cdf-4ad6-9f78-4e624d95c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAILateChunker:\n",
    "    def __init__(self, model: str = \"text-embedding-3-small\"):\n",
    "        \"\"\"\n",
    "        Initialize the chunker with OpenAI client and tokenizer\n",
    "        \"\"\"\n",
    "        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.model = model\n",
    "        self.tokenizer = tiktoken.encoding_for_model(model)\n",
    "        \n",
    "    def chunk_by_tokens(self, input_text: str, chunk_size: int = 512) -> Tuple[List[str], List[Tuple[int, int]]]:\n",
    "        \"\"\"\n",
    "        Split input text into chunks while keeping track of token spans\n",
    "        \n",
    "        Args:\n",
    "            input_text: Text to be chunked\n",
    "            chunk_size: Maximum number of tokens per chunk\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing list of text chunks and their token span annotations\n",
    "        \"\"\"\n",
    "        token_ids = self.tokenizer.encode(input_text)\n",
    "        \n",
    "        chunks = []\n",
    "        span_annotations = []\n",
    "        \n",
    "        for i in range(0, len(token_ids), chunk_size):\n",
    "            chunk_end = min(i + chunk_size, len(token_ids))\n",
    "            if chunk_end - i > 0:\n",
    "                chunk_tokens = token_ids[i:chunk_end]\n",
    "                chunk_text = self.tokenizer.decode(chunk_tokens)\n",
    "                chunks.append(chunk_text)\n",
    "                span_annotations.append((i, chunk_end))\n",
    "        \n",
    "        return chunks, span_annotations\n",
    "    \n",
    "    def get_embeddings(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Get embeddings for the entire text using OpenAI's API\n",
    "        \"\"\"\n",
    "        response = self.client.embeddings.create(\n",
    "            model=self.model,\n",
    "            input=text\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    def late_chunking(self, text_embeddings: List[float], \n",
    "                     span_annotations: List[Tuple[int, int]], \n",
    "                     max_length: int = None) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Perform late chunking by pooling embeddings based on span annotations\n",
    "        \n",
    "        Args:\n",
    "            text_embeddings: Embeddings for the entire text\n",
    "            span_annotations: List of (start, end) token positions for each chunk\n",
    "            max_length: Maximum sequence length to consider\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk embeddings\n",
    "        \"\"\"\n",
    "        embeddings = np.array(text_embeddings)\n",
    "        chunk_embeddings = []\n",
    "        \n",
    "        for start, end in span_annotations:\n",
    "            if max_length is not None:\n",
    "                end = min(end, max_length - 1)\n",
    "                if start >= (max_length - 1):\n",
    "                    continue\n",
    "                    \n",
    "            chunk_size = end - start\n",
    "            if chunk_size >= 1:\n",
    "                chunk_embedding = embeddings[start:end].mean(axis=0) if len(embeddings.shape) > 1 else embeddings\n",
    "                chunk_embeddings.append(chunk_embedding)\n",
    "                \n",
    "        return chunk_embeddings\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "        vec1, vec2 = np.array(vec1), np.array(vec2)\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3064bce0-5c29-4f49-9e75-8bbf812c24c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks created:\n",
      "\n",
      "Chunk 0:\n",
      "As Weaviate celebrates its fifth anniversary, we've had the privilege of collaborating with tens of thousands of developers, gaining invaluable insights into the evolving landscape of AI projects and strategies. Our users constantly push the boundaries of what’s possible. As they continue to scale their applications in production, they guide the evolution of our product and the market itself. The need for optionality One of the main reasons developers choose Weaviate is the optionality it offers in terms of machine learning models, frameworks, and deployment. With new AI models and tools emerging daily, it's crucial to build systems that allow flexibility for tech stacks to evolve. This option\n",
      "\n",
      "Chunk 1:\n",
      "ality, combined with ease of use, helps teams scale AI prototypes into production faster. Flexibility is also vital when it comes to architecture. Different use cases have different requirements. For example, we work with many software companies and those operating in regulated industries. They often require multi-tenancy to isolate data and maintain compliance. When building a Retrieval Augmented Generation (RAG) application, using account or user-specific data to contextualize results, data must remain within a dedicated tenant for its user group. Weaviate’s native, multi-tenant architecture shines for customers who need to prioritize data privacy while maintaining fast retrieval and accuracy.\n",
      "\n",
      "Chunk 2:\n",
      " On the other hand, we support some very large scale single-tenant use cases that orient toward real-time data access. Many of these are in e-commerce and industries that compete on speed and customer experience. Even the slightest latency can send their users looking elsewhere. These use cases leverage our HNSW index on hot storage and vector compression to ensure low latency. The point is, there is no one-size-fits-all solution so optionality is key. I’m very proud that through learning from our customers and community, we’re building a solution that supports diverse use cases and the evolving needs of developers. Introducing hot, warm, and\n",
      "\n",
      "Chunk 3:\n",
      " cold storage tiers It’s amazing to see our customers' products gain popularity, attracting more users, and in many cases, tenants. However, as multi-tenant use cases scale, infrastructure costs can quickly become prohibitive. Since multi-tenancy is a core tenet of our architecture, the next logical step for us was to build a way to help customers drive more efficient resource consumption. We’re pleased to offer tenant offloading and hot, warm, and cold storage tiers as part of our latest release. Weaviate users (Open Source and Enterprise Cloud) can now deactivate or offload tenants to less-expensive warm or cold storage\n",
      "\n",
      "Chunk 4:\n",
      " and reactivate them dynamically, based on the unique patterns of their use case. Here’s what it might look like in practice: One of our customers develops an email platform with tens of thousands of users. 80% of their users are only active during a 12-hour window (US business hours). With our new storage tiers, they can offload tenants to cold storage to save on infrastructure costs when users are inactive. When a user comes online, they can quickly warm up the tenant. This way they reduce storage costs while still offering performance that meets the needs of their customers. alt The Weaviate AI Unit To adapt to this\n",
      "\n",
      "Chunk 5:\n",
      " product change and the evolving AI stack, we’ve introduced a new pricing unit to our Enterprise Cloud offering. An AI Unit (AIU) is a Weaviate-specific unit that can be applied to hot, warm, and cold storage tiers and compute costs. AIUs enable customers to better monitor usage and improve budgeting. In addition to resource costs, AIUs will apply to new AI-native Apps as they are released (more on that next). Apps and tools to fuel AI-native development As we continue to listen to our community, it’s clear that developers need an AI-native framework offering not just flexibility, but also modular GUI tools to\n",
      "\n",
      "Chunk 6:\n",
      " interact with their data and accelerate their use cases. We’re excited about a new line of AI-native apps and tools that will help developers and business users accelerate common use cases. Recommender App Our first app is a Recommender service, now in private beta. The Recommender is a fully managed, low-code way to build scalable recommendation systems. It offers configurable endpoints for item-to-item, item-to-user, and user-to-user recommendation scenarios across multimodal data. Sign up for the private beta here, and stay tuned for more Apps updates coming soon. alt Weaviate Cloud Tools Lastly, new Weaviate Cloud Tools give developers and\n",
      "\n",
      "Chunk 7:\n",
      " non-technical users an easier way to manage, explore, and interact with their data within Weaviate Cloud. The Query and Collections tools are available now in the Weaviate Cloud Console. It’s been an exciting few months, and I’m ecstatic to continue learning from our community and empowering developers to build the future of AI-native possibilities. To dive deeper into our latest product updates, join our upcoming webinar. \n"
     ]
    }
   ],
   "source": [
    "chunker = OpenAILateChunker()\n",
    "\n",
    "text = \"\"\"As Weaviate celebrates its fifth anniversary, we've had the privilege of collaborating with tens of thousands of developers, gaining invaluable insights into the evolving landscape of AI projects and strategies. Our users constantly push the boundaries of what’s possible. As they continue to scale their applications in production, they guide the evolution of our product and the market itself. The need for optionality One of the main reasons developers choose Weaviate is the optionality it offers in terms of machine learning models, frameworks, and deployment. With new AI models and tools emerging daily, it's crucial to build systems that allow flexibility for tech stacks to evolve. This optionality, combined with ease of use, helps teams scale AI prototypes into production faster. Flexibility is also vital when it comes to architecture. Different use cases have different requirements. For example, we work with many software companies and those operating in regulated industries. They often require multi-tenancy to isolate data and maintain compliance. When building a Retrieval Augmented Generation (RAG) application, using account or user-specific data to contextualize results, data must remain within a dedicated tenant for its user group. Weaviate’s native, multi-tenant architecture shines for customers who need to prioritize data privacy while maintaining fast retrieval and accuracy. On the other hand, we support some very large scale single-tenant use cases that orient toward real-time data access. Many of these are in e-commerce and industries that compete on speed and customer experience. Even the slightest latency can send their users looking elsewhere. These use cases leverage our HNSW index on hot storage and vector compression to ensure low latency. The point is, there is no one-size-fits-all solution so optionality is key. I’m very proud that through learning from our customers and community, we’re building a solution that supports diverse use cases and the evolving needs of developers. Introducing hot, warm, and cold storage tiers It’s amazing to see our customers' products gain popularity, attracting more users, and in many cases, tenants. However, as multi-tenant use cases scale, infrastructure costs can quickly become prohibitive. Since multi-tenancy is a core tenet of our architecture, the next logical step for us was to build a way to help customers drive more efficient resource consumption. We’re pleased to offer tenant offloading and hot, warm, and cold storage tiers as part of our latest release. Weaviate users (Open Source and Enterprise Cloud) can now deactivate or offload tenants to less-expensive warm or cold storage and reactivate them dynamically, based on the unique patterns of their use case. Here’s what it might look like in practice: One of our customers develops an email platform with tens of thousands of users. 80% of their users are only active during a 12-hour window (US business hours). With our new storage tiers, they can offload tenants to cold storage to save on infrastructure costs when users are inactive. When a user comes online, they can quickly warm up the tenant. This way they reduce storage costs while still offering performance that meets the needs of their customers. alt The Weaviate AI Unit To adapt to this product change and the evolving AI stack, we’ve introduced a new pricing unit to our Enterprise Cloud offering. An AI Unit (AIU) is a Weaviate-specific unit that can be applied to hot, warm, and cold storage tiers and compute costs. AIUs enable customers to better monitor usage and improve budgeting. In addition to resource costs, AIUs will apply to new AI-native Apps as they are released (more on that next). Apps and tools to fuel AI-native development As we continue to listen to our community, it’s clear that developers need an AI-native framework offering not just flexibility, but also modular GUI tools to interact with their data and accelerate their use cases. We’re excited about a new line of AI-native apps and tools that will help developers and business users accelerate common use cases. Recommender App Our first app is a Recommender service, now in private beta. The Recommender is a fully managed, low-code way to build scalable recommendation systems. It offers configurable endpoints for item-to-item, item-to-user, and user-to-user recommendation scenarios across multimodal data. Sign up for the private beta here, and stay tuned for more Apps updates coming soon. alt Weaviate Cloud Tools Lastly, new Weaviate Cloud Tools give developers and non-technical users an easier way to manage, explore, and interact with their data within Weaviate Cloud. The Query and Collections tools are available now in the Weaviate Cloud Console. It’s been an exciting few months, and I’m ecstatic to continue learning from our community and empowering developers to build the future of AI-native possibilities. To dive deeper into our latest product updates, join our upcoming webinar. \"\"\"\n",
    "\n",
    "chunks, span_annotations = chunker.chunk_by_tokens(text, chunk_size=128)\n",
    "\n",
    "print(\"Chunks created:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29d664f7-a106-4074-ae8c-723da11046c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 8\n",
      "Number of chunk embeddings: 8\n"
     ]
    }
   ],
   "source": [
    "text_embeddings = chunker.get_embeddings(text)\n",
    "\n",
    "chunk_embeddings = chunker.late_chunking(text_embeddings, span_annotations)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Number of chunk embeddings: {len(chunk_embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aeb1cfc-7bb2-4022-a57f-44fc24069e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what do customers need to prioritize?\n",
      "\n",
      "Top 2 most relevant chunks:\n",
      "\n",
      "Chunk 7: Similarity = 0.2628\n",
      "Text:  non-technical users an easier way to manage, explore, and interact with their data within Weaviate Cloud. The Query and Collections tools are available now in the Weaviate Cloud Console. It’s been an exciting few months, and I’m ecstatic to continue learning from our community and empowering developers to build the future of AI-native possibilities. To dive deeper into our latest product updates, join our upcoming webinar. \n",
      "\n",
      "Chunk 6: Similarity = 0.2628\n",
      "Text:  interact with their data and accelerate their use cases. We’re excited about a new line of AI-native apps and tools that will help developers and business users accelerate common use cases. Recommender App Our first app is a Recommender service, now in private beta. The Recommender is a fully managed, low-code way to build scalable recommendation systems. It offers configurable endpoints for item-to-item, item-to-user, and user-to-user recommendation scenarios across multimodal data. Sign up for the private beta here, and stay tuned for more Apps updates coming soon. alt Weaviate Cloud Tools Lastly, new Weaviate Cloud Tools give developers and\n"
     ]
    }
   ],
   "source": [
    "query_text = \"what do customers need to prioritize?\"\n",
    "query_embedding = chunker.get_embeddings(query_text)\n",
    "\n",
    "results = []\n",
    "for i, chunk_embedding in enumerate(chunk_embeddings):\n",
    "    similarity = chunker.cosine_similarity(query_embedding, chunk_embedding)\n",
    "    results.append((similarity, i))\n",
    "\n",
    "results.sort(reverse=True)\n",
    "\n",
    "print(f\"Query: {query_text}\\n\")\n",
    "print(\"Top 2 most relevant chunks:\")\n",
    "for similarity, chunk_idx in results[:2]:\n",
    "    print(f\"\\nChunk {chunk_idx}: Similarity = {similarity:.4f}\")\n",
    "    print(f\"Text: {chunks[chunk_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd0792a-b449-4c19-a74a-c15cd0b6bf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6b9b3eb-4324-4e8b-b949-fd93c3e7262c",
   "metadata": {},
   "source": [
    "# METHOD 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f63769f1-45d3-4525-8db8-e020ce90ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install semchunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cde758c5-24e8-48aa-a902-af5d6a18a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from openai import OpenAI\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5b2d056-62fa-44e0-a5e6-e7a423debebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text: str) -> list[float]:\n",
    "    \"\"\"Get embeddings for a text using OpenAI's embedding model.\"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def cosine_similarity(vec1: list[float], vec2: list[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    vec1, vec2 = np.array(vec1), np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a0f6274-e1c6-41d6-9551-62f642697f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text - we can replace this as we test\n",
    "# text = \"\"\"As Weaviate celebrates its fifth anniversary, we've had the privilege of collaborating with tens of thousands of developers, gaining invaluable insights into the evolving landscape of AI projects and strategies. Our users constantly push the boundaries of what’s possible. As they continue to scale their applications in production, they guide the evolution of our product and the market itself. The need for optionality One of the main reasons developers choose Weaviate is the optionality it offers in terms of machine learning models, frameworks, and deployment. With new AI models and tools emerging daily, it's crucial to build systems that allow flexibility for tech stacks to evolve. This optionality, combined with ease of use, helps teams scale AI prototypes into production faster. Flexibility is also vital when it comes to architecture. Different use cases have different requirements. For example, we work with many software companies and those operating in regulated industries. They often require multi-tenancy to isolate data and maintain compliance. When building a Retrieval Augmented Generation (RAG) application, using account or user-specific data to contextualize results, data must remain within a dedicated tenant for its user group. Weaviate’s native, multi-tenant architecture shines for customers who need to prioritize data privacy while maintaining fast retrieval and accuracy. On the other hand, we support some very large scale single-tenant use cases that orient toward real-time data access. Many of these are in e-commerce and industries that compete on speed and customer experience. Even the slightest latency can send their users looking elsewhere. These use cases leverage our HNSW index on hot storage and vector compression to ensure low latency. The point is, there is no one-size-fits-all solution so optionality is key. I’m very proud that through learning from our customers and community, we’re building a solution that supports diverse use cases and the evolving needs of developers. Introducing hot, warm, and cold storage tiers It’s amazing to see our customers' products gain popularity, attracting more users, and in many cases, tenants. However, as multi-tenant use cases scale, infrastructure costs can quickly become prohibitive. Since multi-tenancy is a core tenet of our architecture, the next logical step for us was to build a way to help customers drive more efficient resource consumption. We’re pleased to offer tenant offloading and hot, warm, and cold storage tiers as part of our latest release. Weaviate users (Open Source and Enterprise Cloud) can now deactivate or offload tenants to less-expensive warm or cold storage and reactivate them dynamically, based on the unique patterns of their use case. Here’s what it might look like in practice: One of our customers develops an email platform with tens of thousands of users. 80% of their users are only active during a 12-hour window (US business hours). With our new storage tiers, they can offload tenants to cold storage to save on infrastructure costs when users are inactive. When a user comes online, they can quickly warm up the tenant. This way they reduce storage costs while still offering performance that meets the needs of their customers. alt The Weaviate AI Unit To adapt to this product change and the evolving AI stack, we’ve introduced a new pricing unit to our Enterprise Cloud offering. An AI Unit (AIU) is a Weaviate-specific unit that can be applied to hot, warm, and cold storage tiers and compute costs. AIUs enable customers to better monitor usage and improve budgeting. In addition to resource costs, AIUs will apply to new AI-native Apps as they are released (more on that next). Apps and tools to fuel AI-native development As we continue to listen to our community, it’s clear that developers need an AI-native framework offering not just flexibility, but also modular GUI tools to interact with their data and accelerate their use cases. We’re excited about a new line of AI-native apps and tools that will help developers and business users accelerate common use cases. Recommender App Our first app is a Recommender service, now in private beta. The Recommender is a fully managed, low-code way to build scalable recommendation systems. It offers configurable endpoints for item-to-item, item-to-user, and user-to-user recommendation scenarios across multimodal data. Sign up for the private beta here, and stay tuned for more Apps updates coming soon. alt Weaviate Cloud Tools Lastly, new Weaviate Cloud Tools give developers and non-technical users an easier way to manage, explore, and interact with their data within Weaviate Cloud. The Query and Collections tools are available now in the Weaviate Cloud Console. It’s been an exciting few months, and I’m ecstatic to continue learning from our community and empowering developers to build the future of AI-native possibilities. To dive deeper into our latest product updates, join our upcoming webinar. \"\"\"\n",
    "text = \"\"\n",
    "with open('demo_paper_to_chunk.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "chunker = semchunk.chunkerify('gpt-4o', chunk_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3915c4f8-6f87-4c60-a9b0-d646bdd3229d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated chunks:\n",
      "\n",
      "Chunk 0:\n",
      "Offset: (0, 9338)\n",
      "\n",
      "Chunk 1:\n",
      "Offset: (9339, 19185)\n",
      "\n",
      "Chunk 2:\n",
      "Offset: (19186, 25964)\n",
      "\n",
      "Chunk 3:\n",
      "Offset: (25965, 32717)\n",
      "\n",
      "Chunk 4:\n",
      "Offset: (32718, 40961)\n",
      "\n",
      "Chunk 5:\n",
      "Offset: (40962, 49260)\n",
      "\n",
      "Chunk 6:\n",
      "Offset: (49261, 57912)\n",
      "\n",
      "Chunk 7:\n",
      "Offset: (57913, 66572)\n",
      "\n",
      "Chunk 8:\n",
      "Offset: (66573, 73308)\n",
      "\n",
      "Chunk 9:\n",
      "Offset: (73309, 80218)\n",
      "\n",
      "Chunk 10:\n",
      "Offset: (80219, 85609)\n"
     ]
    }
   ],
   "source": [
    "chunks, offsets = chunker(text, offsets=True)\n",
    "\n",
    "print(\"Generated chunks:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    # Chunks can be printed, not printing right now because the output will be too long\n",
    "    print(f\"Offset: {offsets[i]}\")\n",
    "\n",
    "chunk_embeddings = [get_embeddings(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b46f2a0-a0bc-43b7-9be0-874b206a22bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Explain the challenges faced in the eBay dataset for collusion detection and how KnowGraph addresses them.\n",
      "\n",
      "Top relevant chunks:\n",
      "\n",
      "Similarity score: 0.7313\n",
      "Chunk 6:\n",
      "Used in Business Rule, “[feedback amt < 𝑎] ∧\n",
      "[(seller_age < 𝑏 ∨ buyer_age < 𝑐)] ⇒\n",
      "collusion”\n",
      "Business Domain knowledge using labeled attributes of the\n",
      "zip code, price, and gross value of a transaction\n",
      "Used in Business Rule, “[(gmv − price >\n",
      "𝑑] ∧ [billing_zip ≠ delivery_zip] ⇒\n",
      "collusion”\n",
      "5.2 Collusion detection on real-world eBay\n",
      "marketplace dataset\n",
      "In this section, we describe the real-world eBay marketplace dataset\n",
      "and our collusion detection results and analysis in detail.\n",
      "Dataset. We use a large-scale proprietary dataset on real-world\n",
      "marketplace transactions from the popular online shopping website\n",
      "eBay, which has more than 135 million users [76]. The dataset contains transactions from 40 days total, each with around 4 million\n",
      "transactions, collected from January to February of 2022. Each transaction consists of three entities: a seller, a buyer, and an item. These\n",
      "transactions and entities are organized into a knowledge graph,\n",
      "where each entity is a node with bidirectional edge relationships\n",
      "with other entities in the same transaction. The corresponding task\n",
      "aims to train models that predict if a transaction indicates buyerseller collusion, a type of fraud in which buyers and sellers conspire\n",
      "for illegal financial gain. This fraud can involve manipulating prices\n",
      "or exchanging fake feedback to deceive the marketplace, leading to\n",
      "significant financial losses. Due to the scale of the marketplace, this\n",
      "has a financial cost of millions of dollars a year based on proprietary\n",
      "estimates, making automated detection crucial.\n",
      "Labels. Each transaction has ground-truth collusion labels from\n",
      "real collusion cases and additional data based on various buyer,\n",
      "seller, and item features. These include relevant knowledge features\n",
      "such as transaction zip code, item price, and shipping cost, comprising 26 features across the three entities. This is summarized\n",
      "in Fig. 2, which contains example differences between benign and\n",
      "anomalous transactions. The ground-truth collusion labels include\n",
      "a single overall binary classification label and multiple fine-grained\n",
      "class or collusion subclass labels. Subclasses are based on specific\n",
      "instances of collusion, such as collusion from a particular group\n",
      "or those exhibiting specific collusive modus operandi. The number of positive labels is extremely sparse, with only around 1330\n",
      "new collusion cases daily. This makes the exact ratio of positive to\n",
      "negative labels 1 : 3269.371, a case of extreme label imbalance.\n",
      "Settings and baselines. To manage label sparsity in our data, we\n",
      "segment the graph into temporal subgraphs, each spanning 20 consecutive days. The initial model training is conducted on a subgraph\n",
      "containing the first 20 days of transactions, a portion of which is\n",
      "reserved as a test set for evaluation. We use additional 20-day snapshots from subsequent periods to test time-shift generalization in\n",
      "the inductive setting. These later snapshots vary in their degree of\n",
      "temporal overlap with the training snapshot; ’overlap’ here refers\n",
      "to days that are included in both the training snapshot and testing snapshots. In the most challenging setting, the test snapshot\n",
      "comprises the latter 20 days and has no overlapping days with the\n",
      "training set. We evaluate the AUC, Average Precision (AP), and the\n",
      "KnowGraph: Knowledge-Enabled Anomaly Detection via Logical Reasoning on Graph Data CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA\n",
      "Table 5: Overall performance of baselines and KnowGraph.\n",
      "KnowGraph outperformes the baseline DGI [66] in both the\n",
      "transductive and inductive settings. The performance is further improved with PCS framework. We report AUC and\n",
      "average precision with 𝑘 = 0.5, where 𝑘 is the proportion of\n",
      "the total dataset considered when evaluating average precision. We bold results with an improvement larger than 1%.\n",
      "Transductive\n",
      "Method AUC AP TP@0.5FP TP@1FP TP@2FP\n",
      "GCN [40] 0.953 0.723 0.719 0.742 0.790\n",
      "DGI [66] 0.892 0.587 0.717 0.738 0.788\n",
      "KnowGraph 0.978 0.755 0.714 0.745 0.788\n",
      "KnowGraph+PCS 0.968 0.731 0.731 0.757 0.814\n",
      "Inductive\n",
      "Method AUC AP TP@0.5FP TP@1FP TP@2FP\n",
      "GCN [40] 0.501 0.0001 0.0 0.0096 0.0096\n",
      "DGI [66] 0.498 0.0001 0.0 0.0096 0.0096\n",
      "KnowGraph 0.609 0.121 0.0048 0.0096 0.0144\n",
      "KnowGraph+PCS 0.649 0.167 0.0287 0.0383 0.0431\n",
      "True Positive rate at a certain False Positive rate (e.g., TP@0.5FP\n",
      "denotes the true positive rate when the false positive rate is 0.5%).\n",
      "The data is organized into a Heterogeneous Knowledge Graph\n",
      "[28] comprising three core entities in the eBay e-commerce setting – users, items, and transactions. Edges define transactional\n",
      "relationships between buyers and sellers (users) for specific items.\n",
      "The heterogeneous knowledge graph with GCN layers is trained on\n",
      "node graph features with Deep Graph Infomax (DGI) [66] to learn\n",
      "label agnostic node embeddings. Inductive prediction generates\n",
      "heterogeneous node embeddings for the four key entities within\n",
      "the transactional sub-graph: transaction, buyer, seller, and item.\n",
      "These embeddings are combined to form a feature set for the subgraph, which is then input into an XGBoost [10] classifier trained\n",
      "to perform node-level collusion classification. This configuration\n",
      "serves as the primary baseline and main model within KnowGraph,\n",
      "and is also used in the GNN architecture for the knowledge models.\n",
      "It is also the main component in the current automated collusion\n",
      "detection system at eBay, and our overall goal is to improve its\n",
      "performance and generalization. We also compare using only domain knowledge and a simpler GCN [40] baseline trained with\n",
      "supervised learning, which has strong transductive performance\n",
      "but weaker generalization. Since we focus on the inductive setting,\n",
      "it is not used as a model in KnowGraph.\n",
      "eBay Dataset Challenges. For the transductive setting, the model\n",
      "has access to the entire graph during training but not the collusion\n",
      "labels for nodes in the test set. The model must generalize to a new\n",
      "snapshot with an unseen graph for the inductive setting. We control\n",
      "the difficulty of this setting by changing the amount of overlapping\n",
      "days with the training snapshot. We observe that generalization is\n",
      "challenging in this context due to several reasons:\n",
      "Figure 6: AUC of test graph snapshots with decreasing overlap with the training snapshot. KnowGraph consistently outperforms baselines. We observe similar performance regardless of how different the new graph is, indicating KnowGraph is robust to the magnitude of the time shifts.\n",
      "• The high volumes of eBay transactions (4-5 million per day)\n",
      "provide substantial changes to the graph structure and label\n",
      "distribution across graph snapshots.\n",
      "• The noise induced by the inductive sub-graph sampling for\n",
      "graph inference can be significant as the entire graph is too\n",
      "large for tractable message passing.\n",
      "• The extreme label imbalance coupled with collusive participants who are mostly new to the platform, often lacking prior\n",
      "purchasing or selling history, composes fewer anomalous\n",
      "links in the graph.\n",
      "• The dataset does not model dynamic time-varying features\n",
      "for users and items, instead relying on static input node features for each such reference entity within a single snapshot.\n",
      "Due to these challenges, baselines cannot generalize to the inductive setting and have performance close to random guessing.\n",
      "However, while not as effective for the transductive setting, expertdesigned knowledge rules have higher performance due to invariance to this distribution shift.\n",
      "Knowledge models. For the learning component of the framework, we augment the unsupervised embedding model trained on\n",
      "the main classification task with several knowledge models centered around more specific tasks for a total of eight models. Utilizing\n",
      "the fine-grained collusion labels, we train a model for each finegrained class for four models. These labels are derived from specific\n",
      "instances or cases of collusion. Next, we train a second binary collusion model on upsampled data. These models are independently\n",
      "trained XGBoost classifiers on their respective tasks using the same\n",
      "unsupervised graph embeddings. Finally, we use graph features\n",
      "on buyer and seller account age, zip code, and price to construct\n",
      "two additional models. Unlike the other models, these are matrices\n",
      "directly derived from the graph.\n",
      "Knowledge rules. For the reasoning component of the framework,\n",
      "we organize the model’s predictions into logical rules. The models are predicates of the main model and are organized into the\n",
      "following six rules.\n",
      "CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA Andy Zhou et al.\n",
      "Table 6: Final rule weights of KnowGraph applied to collusion detection on the real-world eBay dataset. We observe similar\n",
      "\n",
      "Similarity score: 0.6344\n",
      "Chunk 5:\n",
      "the AUC, Average Precision (AP), and the True Positive rate at a\n",
      "certain False Positive rate (e.g., TP@0.5FP denotes the true positive\n",
      "rate when the false positive rate is 0.5%).\n",
      "We adopt two baselines. The first baseline is the Euler [39] work,\n",
      "where the authors use graph neural networks (with or without a\n",
      "recurrent neural network header) to detect the malicious edges.\n",
      "The inputs to the model include the node and edges within a time\n",
      "window. The GNN processes the information and returns a value\n",
      "for each edge, indicating the probability that it is malicious. The\n",
      "model will be trained with negative sampling. In each training step,\n",
      "the model will be trained to give a low malicious probability for\n",
      "the existing edges and a high malicious probability for a randomly\n",
      "sampled set of non-connected node pairs.\n",
      "The second baseline uses link prediction with an Enclosing\n",
      "Graph (EncG) [88]. EncG extracts the K-hop enclosing subgraph\n",
      "0 10 20 30 40 50 58\n",
      "Days\n",
      "0.825\n",
      "0.850\n",
      "0.875\n",
      "0.900\n",
      "0.925\n",
      "0.950\n",
      "0.975\n",
      "1.000\n",
      "AUC\n",
      "Euler\n",
      "EncG\n",
      "KnowGraph\n",
      "Transductive\n",
      "Inductive\n",
      "Figure 4: Detection AUC of baselines and KnowGraph given\n",
      "different time shifts on the LANL dataset. In the challenging\n",
      "inductive setting (yellow), the baseline performance drops\n",
      "significantly while KnowGraph still maintains high detection performance.\n",
      "for each edge and trains a subgraph classification model to determine whether the corresponding edge is malicious. We use 𝐾 = 2\n",
      "considering the tradeoff between efficiency and effectiveness.\n",
      "Knowledge models. We have three models for this task. First, we\n",
      "have the main model to determine whether the edge is malicious,\n",
      "with the same model architecture as in the Euler work. Second, we\n",
      "have the auth model, which is also a binary edge classifier to determine whether the authentication type of the model is NTLM or not.\n",
      "Finally, we have a EncG model that follows the same methodology\n",
      "in the EncG [88] work to judge the edge maliciousness based on\n",
      "the enclosing graphs.\n",
      "Knowledge rules. We designed three rules for the task, which\n",
      "are as follows: (1) Authentication rule, which states that malicious\n",
      "authentications are likely to be in type NTLM, “[main = Mal] ⇒\n",
      "[auth = NTLM]”. This rule incorporates the human knowledge that\n",
      "NTLM authentication is usually a less secured authentication protocol (compared with, for example, Kerberos), and most malicious\n",
      "authentications are using NTLM authentication. (2) Subgraph rule 1\n",
      "which states that the main model should be malicious if 2hop model\n",
      "predicts a malicious, “[EncG = Mal] ⇒ [main = Mal]”. (3) Subgraph\n",
      "rule 2 which states that the main model should be benign if 2hop\n",
      "model predicts a benign, “[EncG = Benign] ⇒ [main = Benign]”.\n",
      "KnowGraph: Knowledge-Enabled Anomaly Detection via Logical Reasoning on Graph Data CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA\n",
      "Table 3: Detection performance (AUC) of the models and\n",
      "KnowGraph framework with partial knowledge. We observe\n",
      "that KnowGraph with the final reasoning framework indeed\n",
      "achieves the best detection performance.\n",
      "Model Transductive Inductive\n",
      "main 0.9946 0.8973\n",
      "auth 0.9440 0.7950\n",
      "EncG 0.9995 0.8269\n",
      "main+auth 0.9947 0.9003\n",
      "main+EncG 0.9997 0.9006\n",
      "main+auth+EncG 0.9999 0.9112\n",
      "Note that rules (2) and (3) combined indicate that the prediction\n",
      "outputs of the main model and the 2hop model should be consistent.\n",
      "Main Results. Based on our intensive evaluations, we show the\n",
      "model and detection performance of the final reasoning pipeline in\n",
      "Table 2. We observe that in the transductive setting, all models perform well with close performance given similar training and testing\n",
      "data distributions. In particular, the detection performance of the\n",
      "baselines can achieve over 0.99 AUC in the inductive setting. By integrating the model information, we can improve the performance\n",
      "and achieve a close-to-perfect detection performance. However, the\n",
      "model performance drops to below 0.9 under the inductive learning\n",
      "setting with out-of-distribution data. By integrating knowledge\n",
      "rules and reasoning, KnowGraph can better aggregate the information from the models in this OOD scenario and significantly\n",
      "improve detection performance. This showcases the robustness\n",
      "of the reasoning framework against OOD cases and matches our\n",
      "intuition of including human knowledge in the pipeline.\n",
      "In addition, we also show the detection AUC and prediction\n",
      "logits change on the time domain for both Euler and our method\n",
      "in Figure 4 and Figure 5, respectively. We can observe that, as the\n",
      "timestamp goes to the later days, the detection AUC will gradually drop due to the OOD issues. For both Euler and EncG, many\n",
      "benign OOD cases are viewed as malicious by the model, which\n",
      "is not seen in the transductive setting. Moreover, many malicious\n",
      "accesses receive a low malicious score in EncG. By comparison, the\n",
      "distribution of benign and malicious accesses is generally similar\n",
      "in both transductive and inductive settings. We attribute it to the\n",
      "stability of the reasoning framework with human knowledge.\n",
      "Ablation Studies. We show the ablation study of model performance in Table 3. We can observe that every model has a relatively\n",
      "good performance (with transductive AUC larger than 0.9 and inductive AUC larger than 0.8). After combining the models with our\n",
      "reasoning framework, we can further improve the performance\n",
      "in both transductive and inductive settings. Note that the reasoning framework is ubiquitously better than each of its components,\n",
      "showing its superiority over simple methods, which, for example,\n",
      "average the results. In addition, we observe a trend that combining\n",
      "better-performing models can yield better reasoning results, which\n",
      "shows the importance of training better models and designing the\n",
      "reasoning framework.\n",
      "0 10 20 30 40 50 58\n",
      "Days\n",
      "5\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "Prediction Logits\n",
      "Euler\n",
      "Benign accesses\n",
      "malicious accesses\n",
      "Transductive\n",
      "Inductive\n",
      "0 10 20 30 40 50 58\n",
      "Days\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "Prediction Logits\n",
      "EncG\n",
      "Benign accesses\n",
      "malicious accesses\n",
      "Transductive\n",
      "Inductive\n",
      "0 10 20 30 40 50 58\n",
      "Days\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "Prediction Logits\n",
      "KnowGraph\n",
      "Benign accesses\n",
      "malicious accesses\n",
      "Transductive\n",
      "Inductive\n",
      "Figure 5: Model prediction logits on the different time shifts\n",
      "for (top) Euler, (middle) EncG, and (bottom) KnowGraph. Low\n",
      "prediction logits indicate that the access is considered malicious. We can observe that all approaches perform well for\n",
      "the transductive setting; for the inductive setting, it is easier\n",
      "for KnowGraph to separate the malicious accesses by setting\n",
      "a threshold at around -1.5. However, it is difficult to classify\n",
      "benign and malicious accesses based on the prediction logits\n",
      "of Euler and EncG.\n",
      "CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA Andy Zhou et al.\n",
      "Table 4: An overview of the models and knowledge rules designed for collusion detection on the real-world eBay dataset, a\n",
      "heterogeneous graph of marketplace transactions. We design a total of eight data-driven learning models and six knowledge\n",
      "rules to enhance the inference of the main task model. These rules are designed by experts with domain knowledge.\n",
      "Model Description Knowledge Rule\n",
      "Main Main model to predict if a user, transaction, or\n",
      "item node is an instance of collusion\n",
      "Used in all rules as the overall base model\n",
      "Secondary Same objective as the main model, but trained on\n",
      "a larger graph snapshot with balanced sampling\n",
      "Used in the Ensemble Rules: “[main =\n",
      "collusion] ⇒ [secondary = collusion]”\n",
      "and “[secondary = collusion] ⇒ [main =\n",
      "collusion].”\n",
      "Account Takeover Model to predict collusion subclass where one of\n",
      "the colluding parties fraudulently takes control\n",
      "of another account\n",
      "Used in Hierarchy Rule: “[ATO = collusion] ⇒\n",
      "[main = collusion].”\n",
      "Sign-In Model to predict collusion subclass accompanied\n",
      "by some problem indicator related to sign-in\n",
      "Used in Hierarchy Rule: “[sign-in =\n",
      "collusion] ⇒ [main = collusion].”\n",
      "One Day Reg Model to predict collusion subclass where buyers\n",
      "register and purchase an item on the same day\n",
      "Used in Hierarchy Rule: “[one-day-reg =\n",
      "collusion] ⇒ [main = collusion].”\n",
      "US eBay Decline Model to predict collusion subclass where the\n",
      "transaction is declined by eBay but overridden by\n",
      "the colluding party\n",
      "Used in Hierarchy Rule, “[ebay-decline =\n",
      "collusion] ⇒ [main = collusion].”\n",
      "Account Age Domain knowledge using labeled attributes on\n",
      "the age and amount of user feedback of an account\n"
     ]
    }
   ],
   "source": [
    "def find_relevant_chunks(query: str, chunks: list[str], chunk_embeddings: list[list[float]], top_k: int = 2) -> list[tuple[float, int, str]]:\n",
    "    \"\"\"Find the most relevant chunks for a given query.\"\"\"\n",
    "    query_embedding = get_embeddings(query)\n",
    "    \n",
    "    similarities = []\n",
    "    for i, chunk_embedding in enumerate(chunk_embeddings):\n",
    "        similarity = cosine_similarity(query_embedding, chunk_embedding)\n",
    "        similarities.append((similarity, i, chunks[i]))\n",
    "    \n",
    "    similarities.sort(reverse=True)\n",
    "    \n",
    "    return similarities[:top_k]\n",
    "\n",
    "query = \"Explain the challenges faced in the eBay dataset for collusion detection and how KnowGraph addresses them.\"\n",
    "\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(\"\\nTop relevant chunks:\")\n",
    "for score, idx, chunk in find_relevant_chunks(query, chunks, chunk_embeddings):\n",
    "    print(f\"\\nSimilarity score: {score:.4f}\")\n",
    "    print(f\"Chunk {idx}:\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd86f7-49d7-4f07-85ff-9f793760ab10",
   "metadata": {},
   "source": [
    "# We can observe the output above has answer to the question being asked. Method 4 seems best till now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4113082-8163-4f19-b576-b1457dbb5d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
